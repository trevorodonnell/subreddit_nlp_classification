{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/merged.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22297, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping unnecessary columns, rearranging position of remaining columns, and reseting index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop(['author', 'num_comments', 'score', 'is_self', 'timestamp', 'year', 'month'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,['fulltext','subreddit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fulltext</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Closing time.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drama with hooking up at work I hooked up with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Customers don't Read #1: Soda Water Eggs Chips...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Any tips for. Newbie on memorizing a menu?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, I'm not giving you another free giveaway. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22292</th>\n",
       "      <td>That's right Sir, it's my fault you can't use ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22293</th>\n",
       "      <td>Blame the \"unfriendly cashier\", it's not like ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22294</th>\n",
       "      <td>Funny little story from my time in retail. Sho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22295</th>\n",
       "      <td>Sons of Un-Ar-Key and explaining how Buy 3, Ge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22296</th>\n",
       "      <td>With the Christmas shopping season up and runn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22297 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                fulltext  subreddit\n",
       "0                                         Closing time.           0\n",
       "1      Drama with hooking up at work I hooked up with...          1\n",
       "2      Customers don't Read #1: Soda Water Eggs Chips...          0\n",
       "3            Any tips for. Newbie on memorizing a menu?           1\n",
       "4      No, I'm not giving you another free giveaway. ...          0\n",
       "...                                                  ...        ...\n",
       "22292  That's right Sir, it's my fault you can't use ...          0\n",
       "22293  Blame the \"unfriendly cashier\", it's not like ...          0\n",
       "22294  Funny little story from my time in retail. Sho...          0\n",
       "22295  Sons of Un-Ar-Key and explaining how Buy 3, Ge...          0\n",
       "22296  With the Christmas shopping season up and runn...          0\n",
       "\n",
       "[22297 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokens = tokenizer.tokenize(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring predictor variable \"fulltext\", where each observation (row) represents the combination of title and post text for a given post. Declaring target variable \"subreddit\", where each observation is a binarized representation of whether the text comes from \"TalesFromYourServer\" reddit (1), or the \"TalesFromRetail\" reddit (0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['fulltext']\n",
    "y = df['subreddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the the normalized value counts from our data to see how balanced our data classes are. Luckily, our data is almost perfectly balanced each with each value having close to 50% of the total count, which will act as our baseline score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.501278\n",
       "1    0.498722\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our test data with a random state of – with an arbitrary number, in this case, 42 – to making our answers replicable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating our TFIDF (term frequency-inverse document frequency) vectorizer. With foundations similar to a CountVectorizer this goes beyond the standard bag-of-words approach, increasing the value of a tokenized word if occurs it frequently in fewer documents, while decreasing the value of more ubiquitous terms. We will use english arguement for our stop_words parameter, removing common English words based on a predetermined list which stored in the TFIDF transformer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer(stop_words='english', strip_accents = 'ascii')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit and transform our vectorizer over our training predictor variables, while just transforming our testing predictor data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tvec = tvec.fit_transform(X_train)\n",
    "X_test_tvec = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two seperate dataframes, and by using the .todense method, are able to save memory by consolidating the sparse matrix which is currently representing our vectorized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tvec_df = pd.DataFrame(X_train_tvec.todense(),columns=tvec.get_feature_names())\n",
    "X_test_tvec_df = pd.DataFrame(X_test_tvec.todense(),columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check the shape of the training and testing data to make sure they have the same amount of features, and their rows add up to 22,297. Also checking for nulls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16722, 37332)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5575, 37332)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tvec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tvec_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_tvec_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating a logistic regression model, fitting the model over our training data, and then scoring it on both the training and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train_tvec_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our logistic regression model scored close to 94% accuracy on our training data, and approximately 91% on our testing data. These are pretty good scores considering we are using unscaled variables without adjusting any of the logistic regression model hyperparameters. Furthermore we can also tell that our model is overfit, with the testing data's accuracy score four percent lower than the training data; which suggests our model suffers due to high variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9372084678866164"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_train_tvec_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9104932735426009"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_test_tvec_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I delcare coefficients as a concatenation of a the column names (the vectorized words), and their corresponding, exponentiated coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = pd.concat([pd.DataFrame(X_train_tvec_df.columns), \n",
    "                          pd.DataFrame(np.transpose(np.exp(logreg.coef_)))],\n",
    "                          axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both columns have the same name so I have to use a function to give them unique names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_column_uniquify(df):\n",
    "    df_columns = df.columns\n",
    "    new_columns = []\n",
    "    for item in df_columns:\n",
    "        counter = 0\n",
    "        newitem = item\n",
    "        while newitem in new_columns:\n",
    "            counter += 1\n",
    "            newitem = \"{}_{}\".format(item, counter)\n",
    "        new_columns.append(newitem)\n",
    "    df.columns = new_columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = df_column_uniquify(coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([0, '0_1'], dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = coefficients.rename(columns={0 : \"word\", \"0_1\" : \"coefficient_value\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients dataframe is put through the function, which differentiates our columns, and sort our values becomes possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sort our coefficients by value. The first list sorts the coefficients in desending order. The second places them in descending order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients that best indicate whether a reddit post was written by a server instead of a retail worker are perhaps obvious, and certainly telling. Servers distinguish themselves by refering to their place of employment (restaurant), themselves (server), the way the reference their cusomters (table), and their incentives (tip and bar). \n",
    "\n",
    "Terms that are most indictative of not being written by a server (and therefore a retail worker) also include a lot of innocous work references, but are indeed more specific to the realm of the retail domain. The place of work (store), the type of work they are doing (retail), how they refer to their cusomters (customers), as well as their positions and objects they interact with. Worth mentioning that \"lady\" is more associated with retail than service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficient_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27500</th>\n",
       "      <td>restaurant</td>\n",
       "      <td>22360.267641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29158</th>\n",
       "      <td>server</td>\n",
       "      <td>12785.183319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32438</th>\n",
       "      <td>table</td>\n",
       "      <td>5027.068066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29170</th>\n",
       "      <td>servers</td>\n",
       "      <td>983.481728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33449</th>\n",
       "      <td>tip</td>\n",
       "      <td>639.902982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3930</th>\n",
       "      <td>bar</td>\n",
       "      <td>295.969239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33463</th>\n",
       "      <td>tips</td>\n",
       "      <td>150.831142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32443</th>\n",
       "      <td>tables</td>\n",
       "      <td>145.470156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29184</th>\n",
       "      <td>serving</td>\n",
       "      <td>110.309488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33460</th>\n",
       "      <td>tipping</td>\n",
       "      <td>106.005107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  coefficient_value\n",
       "27500  restaurant       22360.267641\n",
       "29158      server       12785.183319\n",
       "32438       table        5027.068066\n",
       "29170     servers         983.481728\n",
       "33449         tip         639.902982\n",
       "3930          bar         295.969239\n",
       "33463        tips         150.831142\n",
       "32443      tables         145.470156\n",
       "29184     serving         110.309488\n",
       "33460     tipping         106.005107"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients.sort_values(by='coefficient_value', key=abs, ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficient_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31577</th>\n",
       "      <td>store</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27569</th>\n",
       "      <td>retail</td>\n",
       "      <td>0.003048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9081</th>\n",
       "      <td>customer</td>\n",
       "      <td>0.012311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15308</th>\n",
       "      <td>gt</td>\n",
       "      <td>0.027888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6262</th>\n",
       "      <td>cashier</td>\n",
       "      <td>0.036196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26977</th>\n",
       "      <td>register</td>\n",
       "      <td>0.048041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18081</th>\n",
       "      <td>items</td>\n",
       "      <td>0.061540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29534</th>\n",
       "      <td>shop</td>\n",
       "      <td>0.064069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9755</th>\n",
       "      <td>department</td>\n",
       "      <td>0.066944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18935</th>\n",
       "      <td>lady</td>\n",
       "      <td>0.073607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  coefficient_value\n",
       "31577       store           0.000013\n",
       "27569      retail           0.003048\n",
       "9081     customer           0.012311\n",
       "15308          gt           0.027888\n",
       "6262      cashier           0.036196\n",
       "26977    register           0.048041\n",
       "18081       items           0.061540\n",
       "29534        shop           0.064069\n",
       "9755   department           0.066944\n",
       "18935        lady           0.073607"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coefficients.sort_values(by='coefficient_value', key=abs, ascending = True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tvec_df = pd.DataFrame(tvec.fit_transform(X_train).todense(), \n",
    "                          columns=tvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common words between both datasets were reasonably present and widely applicable in both contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD4CAYAAAApWAtMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWs0lEQVR4nO3de5QlZX3u8e/jgOBwGTSQrAGV9oImCDhIgyIXbyyPihFUvOANNMtZOV5JjsZBs45JXJ6AepQTL4kj4YAR9YjoQp0cEREYRG49MNCDgiiMwYHjFRtw4kSG3/lj15ietudG9e7aPfv7WatX137rrdq/etesfuat2rsqVYUkSW08pOsCJElzn2EiSWrNMJEktWaYSJJaM0wkSa3t0HUBXdhzzz1rZGSk6zIkaU5ZsWLFz6tqr+nWDWWYjIyMMDY21nUZkjSnJPnRptZ5mkuS1JphIklqzTCRJLVmmEiSWhvKC/DjayYYWbKs6zKkGbH6tGO7LkFyZiJJaq+TMEkykmRVF+8tSZp5zkwkSa11HiZJHpvk+iTvTPKlJF9PcmuSD0zqc2KS8SSrkpzetL08yYeb5bcnua1ZflySb3dzNJI0nDq9AJ/kicDngdcDi5qfg4F1wC1JPgqsB04HDgHuBr6R5HhgOfDOZldHAb9Isg9wJHD5NO+1GFgMMG/3ae8GIEl6kLqcmewFXAC8pqpWNm0XV9VEVf0G+C6wL3AocGlV/ayq7gfOBY6uqv8H7JpkN+BRwGeBo+kFy++FSVUtrarRqhqdN39Bv49NkoZKl2EyAdwBHDGpbd2k5fX0Zk7ZzD6upDeruYVegBwFHA5cMaOVSpI2q8sw+Q/geOB1SV61mX5XA89IsmeSecCJwGXNuuXAO5rf1wPPAtZV1UTfqpYk/Z5OL8BX1a+BFwJ/AUx77qmq7gJOBS4BbgCuq6oLmtWX0zvFtbyq1tOb6XjxXZJmWScX4KtqNXBAs/wretdFpvZ54aTlz9K7JjK1zw+ZdBqsqp4789VKkrZkKG+ncuA+CxjzFhSSNGM6/56JJGnuM0wkSa0ZJpKk1gwTSVJrhokkqTXDRJLUmmEiSWrNMJEktWaYSJJaM0wkSa0N5e1UxtdMMLJkWddlSH232tsGaZY4M5EktTZQYZLklCTzu65DkrRtBipMgFOAbQqT5oFZkqQOdRYmSXZJsizJDUlWJXkvsDdwSZJLmj4nJhlv1p8+adv7kvxdkquBw5O8Jsk1SVYm+aQBI0mzq8uZyfOAO6vqyVV1AHAGcCfwrKp6VpK9gdOBZwOLgEOTHN9suwuwqqqeCvwCeAVwRFUtovfs+FdPfbMki5OMJRlbv9an+krSTOoyTMaBY5KcnuSoaZ7bfihwaVX9rKruB84Fjm7WrQfOb5afAxwCXJtkZfP6sVPfrKqWVtVoVY3Omz/tE4IlSQ9SZx8NrqrvJzkEeAHw90m+MaVLptlsg980z3zf0O+cqjq1H3VKkrasy2smewNrq+ozwIeApwD3Ars1Xa4GnpFkz+YayInAZdPs6mLghCR/2Oz3EUn27fsBSJJ+p8svLR4IfDDJA8Bvgf8KHA783yR3NddNTgUuoTf7+NequmDqTqrqu0n+GvhGkoc0+3oz8KPZOhBJGnapqq5rmHWjo6M1NjbWdRmSNKckWVFVo9OtG7TvmUiS5iDDRJLUmmEiSWrNMJEktWaYSJJaM0wkSa0ZJpKk1gwTSVJrhokkqTXDRJLUWpf35urM+JoJRpYs67oMaVatPu3YrkvQdsyZiSSpNcNEktSaYSJJam27DJMkQ3ktSJK6stVhkmQkyc1JzkyyKsm5SY5JckWSW5Mc1vx8J8n1ze8nNtuenORLSb7e9P3ApP3+Y5KxJDcl+dtJ7S9o3u/bSf4hydea9l2SnJXk2uZ9jpv0Hucl+Sow9RHAkqQ+2tb/wT8eeBmwGLgWeBVwJPAi4N3A64Cjq+r+JMcA/wN4abPtIuBgYB1wS5KPVtUdwHuq6pfNo3kvTnIQ8H3gk82+bk/yuUk1vAf4VlW9IckewDVJvtmsOxw4qKp+ObXwJIubupm3+17beNiSpM3Z1jC5varGAZLcBFxcVZVkHBgBFgDnJNkPKGDHSdteXFUTzbbfBfYF7gBe3vyh3wFYCOxPb8Z0W1Xd3mz7OZogAJ4LvCjJO5rXOwOPbpYvmi5IAKpqKbAUYKeF+w3f4yUlqY+2NUzWTVp+YNLrB5p9vQ+4pKpenGQEuHQT264HdkjyGOAdwKFVdXeSs+mFQzZTQ4CXVtUtGzUmTwV+vY3HI0maATN9AX4BsKZZPnkr+u9OLwAmkvwR8Pym/WbgsU0gAbxi0jYXAm9NEoAkB7esWZLU0kyHyQeAv09yBTBvS52r6gbgeuAm4Czgiqb934E3AV9P8m3gJ8BEs9n76J0+uzHJqua1JKlDqRrMywdJdq2q+5oZyMeBW6vqIzOx79HR0RobG5uJXUnS0EiyoqpGp1s3yN8zeWOSlfRmLQvofbpLkjSABvbLfc0sZEZmIpKk/hrkmYkkaY4wTCRJrRkmkqTWDBNJUmuGiSSpNcNEktSaYSJJas0wkSS1NrBfWuyn8TUTjCxZ1nUZUmdWn3Zs1yVoO+PMRJLUWudhkmSPJG9qlvdO8sWua5IkbZvOwwTYg97t5qmqO6vqhG7LkSRtq0G4ZnIa8LjmDsG3An9SVQckORk4nt5zUQ4A/ifwUOC19J7a+ILm2fGPo3eL+r2AtcAbq+rm2T4ISRpmgzAzWQL8sKoWAe+csu4A4FXAYcD7gbVVdTBwJfC6ps9S4K1VdQi9RwB/Yro3SbI4yViSsfVrJ6brIkl6kAZhZrI5l1TVvcC9SSaArzbt48BBSXYFng6c1zzFF2Cn6XZUVUvpBQ87LdxvMJ8IJklz1KCHybpJyw9Mev0AvdofAvyqmdVIkjoyCKe57gV2ezAbVtU9wO1JXgaQnifPZHGSpC3rPEyq6hfAFUlWAR98ELt4NfBnSW6g94jf42ayPknSlg3Eaa6qetU0bWcDZ096PTLduqq6HXhefyuUJG3OQITJbDtwnwWMeTsJSZoxnZ/mkiTNfYaJJKk1w0SS1JphIklqzTCRJLVmmEiSWjNMJEmtGSaSpNYME0lSa4aJJKm1obydyviaCUaWLOu6DKlTq72lkGaQMxNJUmtzMkySnJzkY13XIUnqmZNhIkkaLH0JkyQjSW5Ock6SG5N8Mcn8JIckuSzJiiQXJlnY9F+U5Kqm75eTPLxpvzTJGUm+k2RVksOmea+9kpyf5Nrm54h+HJMkadP6OTN5IrC0qg4C7gHeDHwUOKGqDgHOAt7f9P008K6m7zjw3kn72aWqng68qdlmqv8FfKSqDgVeCpw5XTFJFicZSzK2fu1E+6OTJP1OPz/NdUdVXdEsfwZ4N3AAcFESgHnAXUkWAHtU1WVN33OA8ybt53MAVbU8ye5J9pjyPscA+zf7BNg9yW5Vde/kTlW1FFgKsNPC/WoGjk+S1OhnmEz9g30vcFNVHT65sQmTbdnP1NcPAQ6vqn/f9hIlSTOhn6e5Hp1kQ3CcCFwF7LWhLcmOSZ5UVRPA3UmOavq+Frhs0n5e0fQ/Epho+k/2DeAtG14kWTTjRyJJ2qx+zky+B5yU5JPArfSul1wI/EMzG9kBOAO4CTgJ+Kck84HbgNdP2s/dSb4D7A68YZr3eRvw8SQ3NvtcDvx5X45IkjStfobJA1U19Y/6SuDoqR2raiXwtE3s5/yqOnVK/7OBs5vln9PMXiRJ3RjK26kcuM8CxryVhCTNmL6ESVWtpvfJrbb7eWbrYiRJfec34CVJrRkmkqTWDBNJUmuGiSSpNcNEktSaYSJJas0wkSS1ZphIklozTCRJrQ3l7VTG10wwsmRZ12VIA2W1txhSC85MJEmtDXyYJPmbJO/oug5J0qYNfJhIkgbfQIZJkvckuSXJN4EnNm2LklyV5MYkX07y8Kb90iSnJ7kmyfcnPbFRkjRLBi5MkhwCvBI4GHgJcGiz6tPAu6rqIGAceO+kzXaoqsOAU6a0T97v4iRjScbWr5365F9JUhsDFybAUcCXq2ptVd0DfAXYBdijqjY8G/4cNn5i45ea3yuAkel2WlVLq2q0qkbnzV/Qn8olaUgNYpgA1Db2X9f8Xs+QftxZkro0iGGyHHhxkocl2Q34U+DXwN2Troe8FrhsUzuQJM2ugftffFVdl+T/ACuBHwGXN6tOAv4pyXzgNuD13VQoSZoqVdt6RmnuGx0drbGxsa7LkKQ5JcmKqhqdbt0gnuaSJM0xhokkqTXDRJLUmmEiSWrNMJEktWaYSJJaM0wkSa0ZJpKk1gwTSVJrhokkqbWBuzfXbBhfM8HIkmVdlyENrNWnHdt1CZpjnJlIklozTCRJrc35MEmyOsmeXdchScNsTodJknld1yBJ6jBMkvxVkrc1yx9J8q1m+TlJPpPkxCTjSVYlOX3Sdvcl+bskVwOHT2p/WJKvJ3njrB+MJA25Lmcmy4ENj+EdBXZNsiNwJHArcDrwbGARcGiS45u+uwCrquqpVfXtpm1X4KvAZ6vqU9O9WZLFScaSjK1fO9GP45GkodVlmKwADmme874OuJJeqBwF/Aq4tKp+VlX3A+cCRzfbrQfOn7KvC4D/XVWf3tSbVdXSqhqtqtF58xfM7JFI0pDrLEyq6rfAanrPcv8OvWe9Pwt4HPBvm9n0N1W1fkrbFcDzk6QPpUqStqDrC/DLgXc0vy8H/hxYCVwFPCPJns1F9hOByzazn/8O/AL4RF+rlSRNq+swuRxYCFxZVT8BfgNcXlV3AacClwA3ANdV1QVb2NcpwM5JPtDHeiVJ00hVdV3DrBsdHa2xsbGuy5CkOSXJiqoanW5d1zMTSdJ2wDCRJLVmmEiSWjNMJEmtGSaSpNYME0lSa4aJJKk1w0SS1JphIklqzTCRJLW2Q9cFdGF8zQQjS5Z1XYY01FafdmzXJWgGOTORJLU2sGGS5L7m995Jvtgsn5zkY91WJkmaauBPc1XVncAJXdchSdq0gZ2ZbJBkJMmqadqPTXJl8wCt5zbL1yU5L8muXdQqScNq4MNkOkleDCwBXtA0/TVwTFU9BRgD/nKabRYnGUsytn7txOwVK0lDYOBPc03jWcAo8NyquifJC4H9gSuaR8A/FLhy6kZVtRRYCrDTwv2G74lgktRHczFMbgMeCzyB3iwkwEVVdWKnVUnSEJuLp7l+BLwE+HSSJwFXAUckeTxAkvlJntBlgZI0bOZimFBVtwCvBs4DdgdOBj6X5EZ64fLH3VUnScNnYE9zVdWuze/VwAHN8tnA2c3y9fSulQD8EDh0tmuUJPUMbJj004H7LGDMWzlI0oyZk6e5JEmDxTCRJLVmmEiSWjNMJEmtGSaSpNYME0lSa4aJJKk1w0SS1JphIklqzTCRJLU2lLdTGV8zwciSZV2XIakPVnurpE44M5EktTYrYZLk3bPxPpKkbszWzGRWwyTJUJ6+k6SubFWYJHldkhuT3JDkX5KcneSESevva34vTLI8ycokq5IcleQ04GFN27lNv79s1q9KckrTNpLk5iRnNu3nJjkmyRVJbk1yWNNvlyRnJbk2yfVJjmvaT05yXpKvAt+Y0VGSJG3WFv8H3zwa9z3AEVX18ySPAD68ie6vAi6sqvcnmQfMr6rLk7ylqhY1+zsEeD3wVHrPb786yWXA3cDjgZcBi4Frm/0dCbyI3uzm+KaWb1XVG5LsAVyT5JvN+x8OHFRVv5zmOBY3+2Xe7ntt6bAlSdtga2Ymzwa+WFU/B5juD/Uk1wKvT/I3wIFVde80fY4EvlxVv66q+4AvAUc1626vqvGqegC4Cbi4qgoYB0aaPs8FliRZCVwK7Aw8ull30abqq6qlVTVaVaPz5i/YisOWJG2trQmTADWl7f4N2yYJ8FCAqloOHA2sAf4lyes2sb9NWTdp+YFJrx/gP2dRAV5aVYuan0dX1feadb/eiuORJM2wrQmTi4GXJ/kDgOY012rgkGb9ccCOzbp9gZ9W1aeAfwae0vT5bZIdm+XlwPFJ5ifZBXgxcPk21Hwh8NYmxEhy8DZsK0nqgy1eM6mqm5K8H7gsyXrgeuBdwAVJrqEXNhtmBM8E3pnkt8B9wIaZyVLgxiTXVdWrk5wNXNOsO7Oqrk8yspU1vw84o9lf6AXbC7dyW0lSH6R3SWK47LRwv1p40hldlyGpD/wGfP8kWVFVo9OtG8rvYxy4zwLG/AcnSTPG26lIklozTCRJrRkmkqTWDBNJUmuGiSSpNcNEktSaYSJJas0wkSS1ZphIklobym/Aj6+ZYGTJsq7LkKRZ1c9bzTgzkSS1ZphIklob2DBJ8p0Hsc3xSfbvRz2SpE0b2DCpqqc/iM2OBwwTSZplAxsmSe5L8swkX5vU9rEkJzfLpyX5bpIbk3woydOBFwEfTLIyyeM6Kl2Shs6c/DRX8+jgFwN/XFWVZI+q+lWSrwBfq6ovTrPNYmAxwLzd95rdgiVpOzewM5MtuAf4DXBmkpcAa7e0QVUtrarRqhqdN39B3wuUpGEy6GFyPxvXuDNAVd0PHAacT+86yddnvTJJ0u8M+mmuHwH7J9mJXpA8B/h2kl2B+VX1r0muAn7Q9L8X2K2bUiVpeA1ymFRV3ZHkC8CNwK3A9c263YALkuwMBPiLpv3zwKeSvA04oap+ONtFS9IwSlV1XcPvSfIHwHVVtW8/9j86OlpjY2P92LUkbbeSrKiq0enWDdw1kyR7A1cCH+q6FknS1hm401xVdSfwhK7rkCRtvYGbmUiS5h7DRJLUmmEiSWptID/N1W9J7gVu6bqOAbIn8POuixggjsfGHI+NDfN47FtV096PauAuwM+SWzb18bZhlGTM8fhPjsfGHI+NOR7T8zSXJKk1w0SS1NqwhsnSrgsYMI7HxhyPjTkeG3M8pjGUF+AlSTNrWGcmkqQZZJhIklobujBJ8rwktyT5QZIlXdczG5KcleSnSVZNantEkouS3Nr8fvikdac243NLkv/STdX9keRRSS5J8r0kNyV5e9M+rOOxc5JrktzQjMffNu1DOR4ASeYluT7J15rXQzsW22KowiTJPODjwPOB/YETk+zfbVWz4mzgeVPalgAXV9V+wMXNa5rxeCXwpGabTzTjtr24H/hvVfUnwNOANzfHPKzjsQ54dlU9GVgEPC/J0xje8QB4O/C9Sa+HeSy22lCFCb1H/f6gqm6rqv+g9zCt4zquqe+qajnwyynNxwHnNMvn0Hv88Yb2z1fVuqq6nd5TLA+bjTpnQ1XdVVXXNcv30vujsQ/DOx5VVfc1L3dsfoohHY8kjwSOBc6c1DyUY7Gthi1M9gHumPT6x03bMPqjqroLen9ggT9s2odmjJKMAAcDVzPE49Gc1lkJ/BS4qKqGeTzOAP4KeGBS27COxTYZtjDJNG1+NnpjQzFGSXYFzgdOqap7Ntd1mrbtajyqan1VLQIeCRyW5IDNdN9uxyPJC4GfVtWKrd1kmrbtYiwejGELkx8Dj5r0+pHAnR3V0rWfJFkI0Pz+adO+3Y9Rkh3pBcm5VfWlpnlox2ODqvoVcCm98//DOB5HAC9KspreKfBnJ/kMwzkW22zYwuRaYL8kj0nyUHoXz77ScU1d+QpwUrN8EnDBpPZXJtkpyWOA/YBrOqivL5IE+Gfge1X14UmrhnU89kqyR7P8MOAY4GaGcDyq6tSqemRVjdD72/CtqnoNQzgWD8ZQ3TW4qu5P8hbgQmAecFZV3dRxWX2X5HPAM4E9k/wYeC9wGvCFJH8G/BvwMoCquinJF4Dv0vvk05uran0nhffHEcBrgfHmOgHAuxne8VgInNN8CukhwBeq6mtJrmQ4x2M6w/pvY5t4OxVJUmvDdppLktQHhokkqTXDRJLUmmEiSWrNMJEktWaYSJJaM0wkSa39f9j3w7+aHBiOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_tvec_df.sum().sort_values(ascending=False).head(10).plot(kind='barh');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are made through our logistic regression model, using our text data from our testing set, and then stored in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logreg.predict(X_test_tvec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our true negatives, false postives, false negatives, and true positives are declared as our confusion matrix is set up. The confusion matrix is then plot using the plot_confusion_matrix function from sci-kit learn on our testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(logreg, X_test_tvec_df, y_test, cmap='Blues');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric is the ratio of number of correct predictions over the total number of predictions made.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9104932735426009"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Sensitivity) Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sensitivty metric measures the amount of correct positive predictions (in this case, posts from the server subreddit) over all of our possible positive predictions. This is also considered the true positive rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.874076679563841"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is the ratio that tells us the rate at which we correctly predicted posts over all the posts we predicted came from the server subreddit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9463061690784463"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling our data before instantiating and fitting our K-Nearest Neighbors classifier from sci-kit learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate StandardScalar\n",
    "sc = StandardScaler(with_mean=False)\n",
    "\n",
    "sc_X_train = sc.fit_transform(X_train_tvec_df)\n",
    "sc_X_test = sc.transform(X_test_tvec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 5) \n",
    "knn.fit(sc_X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring our model on both the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(sc_X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(sc_X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our KNN model had a training score of 62.5% and a test score 54.0%. Not only do these scores illustrate how wildly little predicitive power our models have, they also suffer from significantly higher variance, making them even more overfit than our logistic regression models. Granted, this model didn't maximize hyperparameter tuning, but given its poor intital performance (and massive computational cost for my puny Mac laptop) we will forgo using this model as our production model and will defer to the previously explored logistic regression model. The logistic regression model has the additional advantage of having interprettable coefficients.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
